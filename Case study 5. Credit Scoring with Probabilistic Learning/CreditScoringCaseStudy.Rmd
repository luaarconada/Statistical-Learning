---
title: "Case Study: Credit Scoring"
subtitle: "MS in Statistics for Data Science"
author: "Javier Nogales"
date: "2024"
output:
  html_document: 
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: no
    toc: no
    toc_depth: 1
  pdf_document:
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: yes
    toc: yes
    toc_depth: 1
editor_options:
  chunk_output_type: console
---
    
```{r global_options, include=T, echo = F}
knitr::opts_chunk$set(echo = T, warning=FALSE, message=FALSE)
```

# Motivation

When a bank receives a loan application, based on the applicant's profile, the bank has to make a decision regarding whether to go ahead with the loan approval or not. Two types of risks are associated with the bank's decision:

- If the applicant has a good credit risk, i.e. is likely to repay the loan, then not approving the loan to the client results in a loss of business to the bank

- If the applicant has a bad credit risk, i.e. is not likely to repay the loan, then approving the loan to the client results in a financial loss to the bank

To minimize loss from the bank's perspective, the bank needs a decision rule regarding who to give approval of the loan and who not to

<center>
<img src="creditscoring.png" width="400"/>
</center>

The example of the image is P(y^=good)=0.71. This client is in the limit, we are going to define the limit in this case study (it may be tau=0.74, a person is considered good if it is very very good).

An applicant's demographic and socio-economic profiles are considered by loan managers before a decision is taken regarding his/her loan application.

<br>

**Goal:** Predict whether or not somebody will experience financial distress in the future 

The credit score is then estimated by the probability of default based on historical data

### Available Data

The Credit Scoring dataset contains data on 10 predictors and the objective is, using these predictors, to predict the probability that somebody will experience financial distress in the next two years.

Data can be found in Kaggle: https://www.kaggle.com/c/GiveMeSomeCredit/overview

### Load useful libraries

```{r}
library(tidyverse)
library(skimr)
library(mice)
library(VIM)
library(GGally)
library(MASS)
library(glmnet)
library(e1071) 
library(rpart)
library(pROC)
library(class)
library(randomForest)
library(caret)
```

# Load and explore the data set

```{r}
dataCredit<-read.csv("credit-scoring.csv", header = TRUE, sep = ",")

glimpse(dataCredit)
```

Variable 'SeriousDlqin2yrs' contains the labels: whether a person experienced 90 days past due delinquency or worse 

Features or explanatory variables or predictors:

- RevolvingUtilizationOfUnsecuredLines: Total balance on credit cards and personal lines of credit except real estate and no installment debt like car loans divided by the sum of credit limits

- age: Age of borrower in years

- NumberOfTime30-59DaysPastDueNotWorse: Number of times borrower has been 30-59 days past due but no worse in the last 2 years.

- DebtRatio: Monthly debt payments, alimony,living costs divided by monthly gross income

- MonthlyIncome: Monthly income

- NumberOfOpenCreditLinesAndLoans: Number of Open loans (installment like car loan or mortgage) and Lines of credit (e.g. credit cards)

- NumberOfTimes90DaysLate: Number of times borrower has been 90 days or more past due.

- NumberRealEstateLoansOrLines: Number of mortgage and real estate loans including home equity lines of credit

- NumberOfTime60-89DaysPastDueNotWorse: Number of times borrower has been 60-89 days past due but no worse in the last 2 years.

- NumberOfDependents: Number of dependents in family excluding themselves (spouse, children etc.)
  
Remove Id column and rename the target

```{r}
dataCredit <- dataCredit[,-1]
names(dataCredit)[1] = "Creditability"
dataCredit$Creditability = as.factor(dataCredit$Creditability)
levels(dataCredit$Creditability)=c("Good", "Bad") #we change it to understand what it means in today's class
```

Split data into training and testing sets 

```{r}
in_train <- createDataPartition(dataCredit$Creditability, p = 0.3, list = FALSE)  # 40% for training 30% is also a good choice
training <- dataCredit[ in_train,]
testing <- dataCredit[-in_train,]
nrow(training) #45001
nrow(testing) #104999

table(training$Creditability)/length(training$Creditability)
```

93% good loans, 7% bad loans: unbalanced dataset (same unbalance with 40% training and 30%). For a naive director (all people classified as good), the accuracy in next week's clients will be 93% (+-2%, some uncertainty). This director is going to spend a lot of money in that 7% of the bad people, he is going to fail with very few people, 7%, but he is going to lose millions (statistically very good, economically a disaster).

### Data cleaning and Feature Engineering

Take a look first:
```{r}
summary(training)
```

Insights?

There is a baby with 0 years (minimun in age) and someone with 109 years (maximun age), most of the clients are between 41 and 632 (1st and 3rd quadrants). We have a lot of missing values in MonthlyIncome

Perform some data exploration for predictors

```{r}
# Insert your code here
```

Some cleaning:

There are strange categories like 96 and 98. We have values from category 0 to 9 or up to 14 depending on the variable, and then categories 96 and 98 appear, which must be a mistake in labeling or something. We replace those values to the most common one (category 0). We have to do it in the training and the test. It is important to check when they are divided, because if we check it on the whole dataset, we may not notice it.

```{r}
table(training$NumberOfTime30.59DaysPastDueNotWorse)
table(training$NumberOfTime60.89DaysPastDueNotWorse)
table(training$NumberOfTimes90DaysLate)
```

Hence, replace coded values "96" and "98"  with 0

```{r}
training$NumberOfTime30.59DaysPastDueNotWorse[training$NumberOfTime30.59DaysPastDueNotWorse==96] <- 0
training$NumberOfTime30.59DaysPastDueNotWorse[training$NumberOfTime30.59DaysPastDueNotWorse==98] <- 0
training$NumberOfTime60.89DaysPastDueNotWorse[training$NumberOfTime60.89DaysPastDueNotWorse==96] <- 0
training$NumberOfTime60.89DaysPastDueNotWorse[training$NumberOfTime60.89DaysPastDueNotWorse==98] <- 0
training$NumberOfTimes90DaysLate[training$NumberOfTimes90DaysLate==96] <- 0
training$NumberOfTimes90DaysLate[training$NumberOfTimes90DaysLate==98] <- 0

testing$NumberOfTime30.59DaysPastDueNotWorse[testing$NumberOfTime30.59DaysPastDueNotWorse==96] <- 0
testing$NumberOfTime30.59DaysPastDueNotWorse[testing$NumberOfTime30.59DaysPastDueNotWorse==98] <- 0
testing$NumberOfTime60.89DaysPastDueNotWorse[testing$NumberOfTime60.89DaysPastDueNotWorse==96] <- 0
testing$NumberOfTime60.89DaysPastDueNotWorse[testing$NumberOfTime60.89DaysPastDueNotWorse==98] <- 0
testing$NumberOfTimes90DaysLate[testing$NumberOfTimes90DaysLate==96] <- 0
testing$NumberOfTimes90DaysLate[testing$NumberOfTimes90DaysLate==98] <- 0

```

Moreover, there is one outlier with 54 loans or lines
```{r}
if(sum(training$NumberRealEstateLoansOrLines==54)>0){
  training<-training[-(which(training$NumberRealEstateLoansOrLines==54)),]
}
```


### Missing values

We are going to deal with the MonthlyIncome NA's which we are going to do it by replacing it with the median (a lit bit better than with the mean in this case because this distribution is very asymmetric, the maximum is almost 2 millions).
In the variable NumberOfDependents, we replace the NA's with 0's.

```{r}
# Insert your code here
training$MonthlyIncome = replace_na(training$MonthlyIncome, median(training$MonthlyIncome, na.rm = TRUE))
training$NumberOfDependents = replace_na(training$NumberOfDependents, 0)
```

We can create a rf imputation model on the training data

And use the imputation model to predict the values of missing data points (expend time, for this class we use our ideas, previous chunk, but we should use this)

```{r, eval=FALSE}
library(mice)
training.imp <- mice(training, m=2, maxit=4, method = "rf" )

# function to impute new observations based on the previous imputation model
source("https://raw.githubusercontent.com/prockenschaub/Misc/master/R/mice.reuse/mice.reuse.R")

testing.imp <- mice.reuse(training.imp, testing, maxit = 1)[[1]]
```

Less sophisticated ideas:

```{r}
# Replace NA in NumberOfDependents with 0
training$NumberOfDependents[is.na(training$NumberOfDependents)] <- 0
testing$NumberOfDependents[is.na(testing$NumberOfDependents)] <- 0

# Imputation with median
training$MonthlyIncome = ifelse(is.na(training$MonthlyIncome), median(training$MonthlyIncome, na.rm=TRUE), training$MonthlyIncome)
# note we impute NA in testing with info from training
testing$MonthlyIncome = ifelse(is.na(testing$MonthlyIncome), median(training$MonthlyIncome, na.rm=TRUE), testing$MonthlyIncome)
```

Even easier but more dangerous:
```{r, eval=FALSE}
na.omit(training)
```

### Transformations to the variables

Let's take a look at the variables to see what we have to do.

```{r}
ggplot(training, aes(x = MonthlyIncome)) + geom_density()
```

We'll take the logarithm to see if we have one outlier or many outliers.

```{r}
ggplot(training, aes(x = log(MonthlyIncome))) + geom_density()
```

We can see something in the left tail and also, the are some infinite values (some people have no salary), we can't eliminate these clients. We move it.

```{r}
ggplot(training, aes(x = log(MonthlyIncome)+2)) + geom_density()
```

We have to take care on the left because we have many outliers.

We should have to do this with every variable to see if we have to transform the rest.

### Correlations between predictors

```{r}
ggcorr(training[,-1], label = T)
```

Moderate correlations

# Modeling

### Logistic regression

Because we have binary classification, we can use the standard glm function in R:

```{r}
# Insert your code here
# In the final model we have to apply all the ideas, but here we use only one.
logit.model = glm(Creditability ~ ., family=binomial(link='logit'), data=training)
summary(logit.model)
# We could also do lda.model
```

Make predictions (posterior probabilities)

```{r}
# Insert your code here
probability <- predict(logit.model, newdata=testing, type = 'response')
head(probability)

prediction <- as.factor(ifelse(probability > 0.5, 'Bad', 'Good')) #0 = Good, 1 = Bad, we defined this at the beginning
head(prediction)
```

Performance: confusion matrix

```{r}
# Insert your code here
confusionMatrix(prediction,testing$Creditability)
```

The accuracy us 93%, we didn't improve the percentage we had at the beginning. The naive director would have both 0's on the second row because he would say that everyone is good. He would have on the top row 93% and 7%. The most dangerous mistake is the right top one, being bad and being classified as good. It is the biggest out of the two errors.

Note the previous threshold=0.5 is indeed an hyper-parameter. How to optimize it?

If the bank is more worried about false Creditable loans (financial loss) than false non-Creditable ones (loss of business), then in the confusion matrix, better to decrease the element (1,2) at the cost of increasing the (2,1)s


# Incorporing economic impact

A naive-manager classifier: all loans are approved, the only error (7%) comes from the false creditable loans (there are no false non-creditable loans)

We can reduce the false positives by decreasing the probability threshold

And we can select the optimal threshold using some specific economic effects

Assume the bank predicts an application to be credit-worthy and it actually turns out to be credit worthy. That implies, for that application, a 12% profit at the end of 2 years.

On the other hand, if the application turns out to be a default, then the loss is 100%

If the bank predicts an application to be non-creditworthy, then the profit is 0% if the application turns out to be a default, but there is an opportunity loss (1% in 2 years) if the application is really credit-worthy.
Table of profits:

| Prediction/Reference | Good  |  Bad  |
| -------------------- | -----:| -----:|
| Good                 |  0.12 |  -1.0 | 12% is the interest rate in 2 years, annual interest rate of 6%
| Bad                  | -0.01 |  0.0  |

For instance, a naive manager would incur a profit per applicant of
$0.12\times0.93 - 1.0\times0.07 - 0.01\times0.0 + 0.0\times0.0 = 0.0416$
The total profit due to the naive director is
$0.12\times97310 - 1\times6006 - 0.01\times671 + 0\times1012 = 11667 - 6006 -6.71 = 5654.29$

This is the profit we should improve

Profit table as a vector:

```{r}
profit.unit <- c(0.12, -0.01, -1.0, 0.0)
```

If we change the threshold from 0.5 to 0.1, we reduce incredibly the cost because the most dangerous error decreases, but the vanilla one increases as a payoff.

If we type 0 in the threshold, everyone is bad, so the first row is both 0's. We don't have any expensive mistake. However, we don't have any profit, we don't make any money. We have to balance the loss and profit to win the maximum money that we can.

## Selecting the optimal threshold to give the loan

```{r}
profit.i = matrix(NA, nrow = 50, ncol = 10)
# 100 replicates for training/testing sets for each of the 10 values of threshold

p0=0.8
p1=1-p0

j <- 0
for (threshold in seq(0.05,0.5,0.05)){
  #for (p1 in c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.75, 0.8, 0.9)){
  
  j <- j + 1
  cat(j)
  for(i in 1:50){
    
    # partition data intro training (40%) and testing sets (60%)
    d <- createDataPartition(training$Creditability, p = 0.4, list = FALSE)
    # select training sample
    
    train<-training[d,]
    test <-training[-d,]  
    
    #p1=1-p0
    
    # consider the lda classifier, but any other classifier should be considered
    full.model <- lda(Creditability ~ ., data=train, prior = c(p0, p1)) #here we write our favourite model, we started with logistic regression, but now changed to lda because he has tried all the models and has seen that this is the best.
    
    # posterior probabilities
    probability = predict(full.model, test)$posterior
    
    # Predictions with a given threshold
    Cred.pred = rep("Good", nrow(test))
    Cred.pred[which(probability[,2] > threshold)] = "Bad"
    
    CM = confusionMatrix(factor(Cred.pred), test$Creditability)$table

    profit.applicant <- sum(profit.unit*CM)/sum(CM) #We don't need the denominator, but we like to see it as a percentage, the important thing is the numerator
    profit.i[i,j] <- profit.applicant
  }
}
```

Summary of economic value of predictions

```{r}
boxplot(profit.i, main = "Hyper-parameter selection",
        ylab = "unit profit",
        xlab = "threshold",names = seq(0.05,0.5,0.05),col="royalblue2")
```

When is a client good? When the probability of being good is less than 85%.

Threshold values around 0.15 are reasonable:

```{r}
apply(profit.i, 2, median) #The third one is the corresponding to 0.15 and it is in fact the highest one.
```


### Final prediction for testing set using the optimal hyper-parameter:

```{r}
lda.model <- lda(Creditability ~ ., data=training, prior = c(.8, .2))
probability = predict(lda.model, newdata=testing)$posterior
threshold = 0.15
Cred.pred = rep("Good", nrow(testing))
Cred.pred[which(probability[,2] > threshold)] = "Bad"
CM = confusionMatrix(factor(Cred.pred), testing$Creditability)$table
profit.applicant <- sum(profit.unit*CM)/sum(CM)
profit.applicant
```

If the average loan amount is 15000 euros, and there are over 10000 applicants in one month then this is the expected profit in 2 years for the applicants in one month:

```{r}
profit.applicant*15000*10000
```


# Machine-Learning tools

## kNN

Using first the original package

```{r}
# if a tool is expensive with n, we can always reduce n
knn_pred <- knn(train = scale(training[1:10000,-1]), test = scale(testing[1:10000,-1]), cl = training$Creditability[1:10000], k=5)

confusionMatrix(knn_pred, testing$Creditability[1:10000])

```

How to choose the hyper-parameter k?

Incorporating a custom metric in Caret

```{r}
EconomicProfit <- function(data, lev = NULL, model = NULL) 
{
  y.pred = data$pred 
  y.true = data$obs
  CM = confusionMatrix(y.pred, y.true)$table
  out = sum(profit.unit*CM)/sum(CM)
  names(out) <- c("EconomicProfit")
  out
}
```

Define the control to optimize the hyper-parameters

Note we need to include there the definition of our profit function

```{r}
ctrl <- trainControl(method = "cv", number = 5,
                     classProbs = TRUE, 
                     summaryFunction = EconomicProfit,
                     verboseIter=T)
```

Train

```{r}
knnFit <- train(Creditability ~ ., 
                method = "knn", 
                data = training,
                preProcess = c("center", "scale"),
                tuneLength = 7,
                metric = "EconomicProfit",
                trControl = ctrl)
print(knnFit)
```

Predictions

```{r}
knnPred = predict(knnFit, testing)

confusionMatrix(knnPred,testing$Creditability)

EconomicProfit(data = data.frame(pred  = knnPred, obs = testing$Creditability))

```

Not too good

Let's predict using probabilities instead. kNN estimates the probabilities using the proportion of votes in the neighbours, but take care: the less the number of neighbours the less accurate the probabilities. And sometimes, to predict better kNN selects a low number of neighbours...

```{r}
knnProb = predict(knnFit, testing, type="prob")
head(knnProb)

threshold = 0.2
Cred.pred = rep("Good", nrow(testing))
Cred.pred[which(knnProb[,2] > threshold)] = "Bad"
CM = confusionMatrix(factor(Cred.pred), testing$Creditability)$table
profit.applicant <- sum(profit.unit*CM)/sum(CM)
profit.applicant
```

A bit better

## SVM

The most known code is in the e1071 library

Train

```{r}
# if a tool is expensive with n, we can always reduce n
svm.train <- svm(Creditability ~., data=training[1:10000,], scale=T, kernel="radial",
                 gamma=0.01, cost=1)
# gamma is the parameter of the radial basis function
```

Prediction

```{r}
svm.pred <- predict(svm.train, newdata=testing)
confusionMatrix(svm.pred, testing$Creditability)
EconomicProfit(data = data.frame(pred  = svm.pred, obs = testing$Creditability))
```

How to select the hyper-parameters gamma and cost?

Train and tune using Caret:

```{r}
svmFit <- train(Creditability ~., method = "svmRadial", 
                data = training[1:10000,],
                preProcess = c("center", "scale"),
                tuneGrid = expand.grid(C = c(.25, .5, 1),
                                      sigma = c(0.01,.05)), 
                metric = "EconomicProfit",
                trControl = ctrl)
print(svmFit)
```

Predictions

```{r}
svmPred = predict(svmFit, testing)
confusionMatrix(svmPred,testing$Creditability)
EconomicProfit(data = data.frame(pred  = svmPred, obs = testing$Creditability))

```

The performance is not good

Let's use now the probabilities from SVM: they are calibrated using Platt scaling (logistic regression on the SVM’s scores)

```{r}
svmProb = predict(svmFit, testing, type="prob")

threshold = 0.2
Cred.pred = rep("Good", nrow(testing))
Cred.pred[which(svmProb[,2] > threshold)] = "Bad"
CM = confusionMatrix(factor(Cred.pred), testing$Creditability)$table
profit.applicant <- sum(profit.unit*CM)/sum(CM)
profit.applicant
```

Only a bit better...

## Decision trees

Using the original package

```{r}
rpart.fit <- rpart(Creditability ~., method="class", data = training)
summary(rpart.fit)
```

For each node in the tree, the number of examples reaching the decision point is listed

Visualizing decision trees

```{r}
library(rpart.plot)
rpart.plot(rpart.fit, digits = 3, fallen.leaves = TRUE,
           type = 3, extra=101)
```

Predictions

```{r}
DTPred <- predict(rpart.fit, testing, type="class")
EconomicProfit(data = data.frame(pred  = DTPred, obs = testing$Creditability))
```

A bit weak model...

## C5.0

Advanced DT model, somehow it's a boosting approach 

Train and tune using Caret:

```{r}
grid_c50 <- expand.grid( .winnow = c(TRUE,FALSE), .trials=c(1,5,10,15), .model="tree" )

fit.c50 <- train(Creditability ~.,
                data=training,
                method="C5.0",
                metric="EconomicProfit",
                tuneGrid = grid_c50,
                trControl = ctrl)
print(fit.c50)
```

Winnowing is a feature selection step conducted before modelling
and trials = number of boosting iterations

Predictions

```{r}
c50.pred <- predict(fit.c50, newdata=testing)
confusionMatrix(c50.pred, testing$Creditability)
EconomicProfit(data = data.frame(pred  = c50.pred, obs = testing$Creditability))              
```

A bit weak...

Finally, let's use probabilities: fraction of samples of the same class in a leaf

```{r}
c50.Prob = predict(fit.c50, testing, type="prob")

threshold = 0.2
Cred.pred = rep("Good", nrow(testing))
Cred.pred[which(c50.Prob[,2] > threshold)] = "Bad"
CM = confusionMatrix(factor(Cred.pred), testing$Creditability)$table
profit.applicant <- sum(profit.unit*CM)/sum(CM)
profit.applicant
```

Better!