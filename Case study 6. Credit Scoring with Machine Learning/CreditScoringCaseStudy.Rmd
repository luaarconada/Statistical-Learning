---
title: "Case Study: Credit Scoring"
subtitle: "MS in Statistics for Data Science"
author: "Javier Nogales"
date: "2024"
output:
  html_document: 
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: no
    toc: no
    toc_depth: 1
  pdf_document:
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: yes
    toc: yes
    toc_depth: 1
editor_options:
  chunk_output_type: console
---
    
```{r global_options, include=T, echo = F}
knitr::opts_chunk$set(echo = T, warning=FALSE, message=FALSE)
```

# Motivation

When a bank receives a loan application, based on the applicant's profile, the bank has to make a decision regarding whether 
to go ahead with the loan approval or not. Two types of risks are associated with the bank's decision:

- If the applicant has a good credit risk, i.e. is likely to repay the loan, then not approving the loan to the client results in a loss of business to the bank

- If the applicant has a bad credit risk, i.e. is not likely to repay the loan, then approving the loan to the client results in a financial loss to the bank

To minimize loss from the bank's perspective, the bank needs a decision rule regarding who to give approval of the loan and who not to

<center>
<img src="creditscoring.png" width="400"/>
</center>

An applicant's demographic and socio-economic profiles are considered by loan managers before a decision is taken regarding his/her loan application.

<br>

**Goal:** Predict whether or not somebody will experience financial distress in the future 

The credit score is then estimated by the probability of default based on historical data

### Available Data

The Credit Scoring dataset contains data on 10 predictors and the objective is, using these predictors, to predict the probability that somebody will experience financial distress in the next two years.

Data can be found in Kaggle: https://www.kaggle.com/c/GiveMeSomeCredit/overview

### Load useful libraries

```{r}
library(tidyverse)
library(skimr)
library(mice)
library(VIM)
library(GGally)
library(MASS)
library(glmnet)
library(e1071) 
library(rpart)
library(pROC)
library(class)
library(randomForest)
library(caret)
```

# Load and explore the data set

```{r}
dataCredit<-read.csv("credit-scoring.csv", header = TRUE, sep = ",")

glimpse(dataCredit)
```

Variable 'SeriousDlqin2yrs' contains the labels: whether a person experienced 90 days past due delinquency or worse 

Features or explanatory variables or predictors:

- RevolvingUtilizationOfUnsecuredLines: Total balance on credit cards and personal lines of credit except real estate and no installment debt like car loans divided by the sum of credit limits

- age: Age of borrower in years

- NumberOfTime30-59DaysPastDueNotWorse: Number of times borrower has been 30-59 days past due but no worse in the last 2 years.

- DebtRatio: Monthly debt payments, alimony,living costs divided by monthly gross income

- MonthlyIncome: Monthly income

- NumberOfOpenCreditLinesAndLoans: Number of Open loans (installment like car loan or mortgage) and Lines of credit (e.g. credit cards)

- NumberOfTimes90DaysLate: Number of times borrower has been 90 days or more past due.

- NumberRealEstateLoansOrLines: Number of mortgage and real estate loans including home equity lines of credit

- NumberOfTime60-89DaysPastDueNotWorse: Number of times borrower has been 60-89 days past due but no worse in the last 2 years.

- NumberOfDependents: Number of dependents in family excluding themselves (spouse, children etc.)
  
Remove Id column and rename the target

```{r}
dataCredit <- dataCredit[,-1]
names(dataCredit)[1] = "Creditability"
dataCredit$Creditability = as.factor(dataCredit$Creditability)
levels(dataCredit$Creditability)=c("Good", "Bad")
```

Split data into training and testing sets 

```{r}
in_train <- createDataPartition(dataCredit$Creditability, p = 0.4, list = FALSE)  # 40% for training
training <- dataCredit[ in_train,]
testing <- dataCredit[-in_train,]
nrow(training)
nrow(testing)

table(training$Creditability)/length(training$Creditability)
```

93% good loans, 7% bad loans: unbalanced dataset

### Data cleaning and Feature Engineering

Take a look first:
```{r}
summary(training)
```

Insights?

Perform some data exploration for predictors

```{r}
# Insert your code here

```

Some cleaning:

There are strange categories like 96 and 98

```{r}
table(training$NumberOfTime30.59DaysPastDueNotWorse)
table(training$NumberOfTime60.89DaysPastDueNotWorse)
table(training$NumberOfTimes90DaysLate)
```

Hence, replace coded values "96" and "98"  with 0

```{r}
training$NumberOfTime30.59DaysPastDueNotWorse[training$NumberOfTime30.59DaysPastDueNotWorse==96] <- 0
training$NumberOfTime30.59DaysPastDueNotWorse[training$NumberOfTime30.59DaysPastDueNotWorse==98] <- 0
training$NumberOfTime60.89DaysPastDueNotWorse[training$NumberOfTime60.89DaysPastDueNotWorse==96] <- 0
training$NumberOfTime60.89DaysPastDueNotWorse[training$NumberOfTime60.89DaysPastDueNotWorse==98] <- 0
training$NumberOfTimes90DaysLate[training$NumberOfTimes90DaysLate==96] <- 0
training$NumberOfTimes90DaysLate[training$NumberOfTimes90DaysLate==98] <- 0

testing$NumberOfTime30.59DaysPastDueNotWorse[testing$NumberOfTime30.59DaysPastDueNotWorse==96] <- 0
testing$NumberOfTime30.59DaysPastDueNotWorse[testing$NumberOfTime30.59DaysPastDueNotWorse==98] <- 0
testing$NumberOfTime60.89DaysPastDueNotWorse[testing$NumberOfTime60.89DaysPastDueNotWorse==96] <- 0
testing$NumberOfTime60.89DaysPastDueNotWorse[testing$NumberOfTime60.89DaysPastDueNotWorse==98] <- 0
testing$NumberOfTimes90DaysLate[testing$NumberOfTimes90DaysLate==96] <- 0
testing$NumberOfTimes90DaysLate[testing$NumberOfTimes90DaysLate==98] <- 0

```

Moreover, there is one outlier with 54 loans or lines
```{r}
if(sum(training$NumberRealEstateLoansOrLines==54)>0){
  training<-training[-(which(training$NumberRealEstateLoansOrLines==54)),]
}
```


### Missing values

```{r}
# Insert your code here


```

We can create a rf imputation model on the training data

And use the imputation model to predict the values of missing data points

```{r, eval=FALSE}
library(mice)
training.imp <- mice(training, m=2, maxit=4, method = "rf" )

# function to impute new observations based on the previous imputation model
source("https://raw.githubusercontent.com/prockenschaub/Misc/master/R/mice.reuse/mice.reuse.R")

testing.imp <- mice.reuse(training.imp, testing, maxit = 1)[[1]]
```

Less sophisticated ideas:

```{r}
# Replace NA in NumberOfDependents with 0
training$NumberOfDependents[is.na(training$NumberOfDependents)] <- 0
testing$NumberOfDependents[is.na(testing$NumberOfDependents)] <- 0

# Imputation with median
training$MonthlyIncome = ifelse(is.na(training$MonthlyIncome), median(training$MonthlyIncome, na.rm=TRUE), training$MonthlyIncome)
# note we impute NA in testing with info from training
testing$MonthlyIncome = ifelse(is.na(testing$MonthlyIncome), median(training$MonthlyIncome, na.rm=TRUE), testing$MonthlyIncome)
```

Even easier but more dangerous:
```{r, eval=FALSE}
na.omit(training)
```

### Correlations between predictors

```{r}
ggcorr(training[,-1], label = T)
```

Moderate correlations

# Modeling

### Logistic regression

Because we have binary classification, we can use the standard glm function in R:

```{r}
# Insert your code here

logit.model <- 
  
```

Make predictions (posterior probabilities)

```{r}
# Insert your code here

probability <- predict(logit.model,  )
head(probability)

prediction <- 
```

Performance: confusion matrix

```{r}
# Insert your code here

```

Note the previous threshold=0.5 is indeed an hyper-parameter. How to optimize it?

If the bank is more worried about false Creditable loans (financial loss) than false non-Creditable ones (loss of business), then in the confusion matrix, better to decrease the element (1,2) at the cost of increasing the (2,1)


# Incorporing economic impact

A naive-manager classifier: all loans are approved, the only error (7%) comes from the 
false creditable loans (there are no false non-creditable loans)

We can reduce the false positives by decreasing the probability threshold

And we can select the optimal threshold using some specific economic effects

Assume the bank predicts an application to be credit-worthy and 
it actually turns out to be credit worthy. That implies, for that application, 
a 12% profit at the end of 2 years

On the other hand, if the application turns out to be a default, then the loss is 100%

If the bank predicts an application to be non-creditworthy, then the profit is 0% if the application
turns out to be a default, but there is an opportunity loss (1% in 2 years) if the application 
is really credit-worthy

Table of profits:

| Prediction/Reference | Good | Bad  |
| -------------------- | ----:|-----:|
| Good                 | 0.12 | -1.0 |
| Bad                  |-0.01 |  0.0 |

For instance, a naive manager would incur a profit per applicant of
$0.12\times0.93 - 1.0\times0.07 - 0.01\times0.0 + 0.0\times0.0 = 0.0416$

This is the profit we should improve

Profit table as a vector:

```{r}
profit.unit <- c(0.12, -0.01, -1.0, 0.0)
```



## Selecting the optimal threshold to give the loan

```{r}
profit.i = matrix(NA, nrow = 50, ncol = 10)
# 100 replicates for training/testing sets for each of the 10 values of threshold

p0=0.8
p1=1-p0

j <- 0
for (threshold in seq(0.05,0.5,0.05)){
  #for (p1 in c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.75, 0.8, 0.9)){
  
  j <- j + 1
  cat(j)
  for(i in 1:50){
    
    # partition data intro training (40%) and testing sets (60%)
    d <- createDataPartition(training$Creditability, p = 0.4, list = FALSE)
    # select training sample
    
    train<-training[d,]
    test <-training[-d,]  
    
    #p1=1-p0
    
    # consider the lda classifier, but any other classifier should be considered
    full.model <- lda(Creditability ~ ., data=train, prior = c(p0, p1))
    
    # posterior probabilities
    probability = predict(full.model, test)$posterior
    
    # Predictions with a given threshold
    Cred.pred = rep("Good", nrow(test))
    Cred.pred[which(probability[,2] > threshold)] = "Bad"
    
    CM = confusionMatrix(factor(Cred.pred), test$Creditability)$table

    profit.applicant <- sum(profit.unit*CM)/sum(CM)
    profit.i[i,j] <- profit.applicant
    
  }
}

```

Summary of economic value of predictions

```{r}
boxplot(profit.i, main = "Hyper-parameter selection",
        ylab = "unit profit",
        xlab = "threshold",names = seq(0.05,0.5,0.05),col="royalblue2")

```

threshold values around 0.15 are reasonable:

```{r}
apply(profit.i, 2, median) 
```


### Final prediction for testing set using the optimal hyper-parameter:

```{r}
lda.model <- lda(Creditability ~ ., data=training, prior = c(.8, .2))
probability = predict(lda.model, newdata=testing)$posterior
threshold = 0.15
Cred.pred = rep("Good", nrow(testing))
Cred.pred[which(probability[,2] > threshold)] = "Bad"
CM = confusionMatrix(factor(Cred.pred), testing$Creditability)$table
profit.applicant <- sum(profit.unit*CM)/sum(CM)
profit.applicant
```

If the average loan amount is 15000 euros, and there are over 10000 applicants in one month then this is the expected profit in 2 years for the applicants in one month:

```{r}
profit.applicant*15000*10000
```


# Machine-Learning tools

## kNN

Using first the original package

```{r}
# if a tool is expensive with n, we can always reduce n
knn_pred <- knn(train = scale(training[1:10000,-1]), test = scale(testing[1:10000,-1]), cl = training$Creditability[1:10000], k=5)
head(knn_pred)

confusionMatrix(knn_pred, testing$Creditability[1:10000])
```

How to choose the hyper-parameter k? If instead of k = 5 (93'31%) we take k = 7, the accuracy increases a bit (93'39%). With k = 9  even higher (93'51%). The higher the k the higher the accuracy.

train = scale(training[1:10000,-1]) is used to train with a low part of the training set because our original one is very big in observations.

The top right error is worse than the other (classifying a bad person as good), the bank will lose more money. We want to decrease this error and as a payoff the other error will increase, we do this to decrease the loss of money (increase the profit). We change the threshold to do that.

Incorporating a custom metric in Caret

```{r}
# We have to add lev = NULL and model = NULL to make caret work 
EconomicProfit <- function(data, lev = NULL, model = NULL) 
{
  y.pred = data$pred 
  y.true = data$obs
  CM = confusionMatrix(y.pred, y.true)$table
  out = sum(profit.unit*CM)/sum(CM) # The profit expression
  names(out) <- c("EconomicProfit")
  out # Important to use this name so caret think it is like accuracy
}
```

Define the control to optimize the hyper-parameters

Note we need to include there the definition of our profit function

```{r}
ctrl <- trainControl(method = "cv", number = 5,
                     classProbs = TRUE, 
                     summaryFunction = EconomicProfit,
                     verboseIter=T)
```

Train (it takes a really long time, 5 folds for 6 different k)

```{r}
knnFit <- train(Creditability ~ ., 
                method = "knn", 
                data = training,
                preProcess = c("center", "scale"),
                tuneLength = 7, # k = 7
                metric = "EconomicProfit",
                trControl = ctrl)
print(knnFit)
```

Predictions

```{r}
knnPred = predict(knnFit, testing) # We predict that  a person is badi if P(y = bad) >0.5 (internally the threshold is always 0.5)

confusionMatrix(knnPred,testing$Creditability)

EconomicProfit(data = data.frame(pred  = knnPred, obs = testing$Creditability))

```

Not too good

Let's predict using probabilities instead. kNN estimates the probabilities using the proportion of votes in the neighbours, but take care: the less the number of neighbours the less accurate the probabilities. And sometimes, to predict better kNN selects a low number of neighbours...

```{r}
knnProb = predict(knnFit, testing, type="prob") #prob to get the probabilities instead of the class predictions
head(knnProb) #We get the good and the bad probability (we only need one)

threshold = 0.2 # Now we change manually the threshold, instead of 50%, 20%. Now a person is bad if P(y = bad) > 0.2 (more conservative to minimize the most dangeorus mistake)
Cred.pred = rep("Good", nrow(testing))
Cred.pred[which(knnProb[,2] > threshold)] = "Bad"
CM = confusionMatrix(factor(Cred.pred), testing$Creditability)$table
profit.applicant <- sum(profit.unit*CM)/sum(CM)
profit.applicant
```

A bit better profit

## Thresholder in caret

Useful if we do not have economic information

```{r}
thres <- thresholder(knnFit, threshold = seq(.1,0.6, by = 0.1), final = TRUE, statistics = "all") # Error
# Caret optimizes the threshold, but we cannot do it with the profit, we optimize the sensitivity or specificity or Kappa or Accuracy or precision or... (one of the columns). It gives the value of all the measure according to the threshold imputed, we look the maximum number in the column we are interested in and we see what the corresponding threshold is

thres
```


# The ROC curve

ROC curve shows true positives vs false positives in relation with different thresholds:

- y-axis = Sensitivity (TP)
- x-axis = Specificity (1-FP)

The ROC:

```{r}
library(pROC)
# Each point in the ROC curve is a thresholf, we want the closest to the top left corner

roc.knn=roc(testing$Creditability ~ knnProb[,2])

plot(roc.knn, col="red",print.thres=TRUE)

roc.knn$auc
```

The best threshold is that with the highest sum sensitivity + specificity. We choose as a our threshold 0.118 (black dot), so a person is bad if P(y^ = bad) > 0.118.
 

AUC = Area Under the Curve: around 0.72, the larger the better. Maximum is 1. The larger the AUC, the better for the model, the closest to 1 the better. If it is near 0.5 it is a quite bad model. In our case it is 0.7173 (not good, but not bad, more or less).

Seems a threshold around 0.1 is reasonable

- this is because classes are unbalanced

- this is the threshold with the best balance between sensitivity and specificity


## SVM

The most known code is in the e1071 library. It is very expensive for a very big dataset. This is why he chooses 10000 observations by chance in the code

Train

```{r}
# if a tool is expensive with n, we can always reduce n
svm.train <- svm(Creditability ~., data=training[1:10000,], scale=T, kernel="radial", # we select the kernel (there are may)
                 gamma=0.01, cost=1) #cost is the hyperparameter C we had in the notes
# gamma is the parameter of the radial basis function
```

Prediction

```{r}
svm.pred <- predict(svm.train, newdata=testing)
confusionMatrix(svm.pred, testing$Creditability)
EconomicProfit(data = data.frame(pred  = svm.pred, obs = testing$Creditability))
```

Accuracy is 93.5% and economic profit is 0.04858821.

How to select the hyper-parameters gamma and cost?

Train and tune using Caret:

```{r}
svmFit <- train(Creditability ~., method = "svmRadial", # We can treat the method as a hyperparameter, we have to do it by hand
                data = training[1:10000,],
                preProcess = c("center", "scale"),
                tuneGrid = expand.grid(C = c(.25, .5, 1), # grid for C
                                      sigma = c(0.01,.05)), # grid for sigma (our previous gamma)
                metric = "EconomicProfit", # maximizing the profit again
                trControl = ctrl)
# We do 5 foldd and in each fold 3 C's and 2 sigmas, so 5*3*2 = 30 iterations. Tipycally it would be 5 for each of the hyperparameters. For our project the grid should be bigger than the one here in class. 5 each at least
print(svmFit)
```

Predictions

```{r}
svmPred = predict(svmFit, testing)
head(svmPred)
confusionMatrix(svmPred,testing$Creditability)
EconomicProfit(data = data.frame(pred  = svmPred, obs = testing$Creditability))

```

The performance is not good, the accuracy is 93.45% but the economic profit is 0.04937388.

Let's use now the probabilities from SVM: they are calibrated using Platt scaling (logistic regression on the SVMâ€™s scores)

```{r}
svmProb = predict(svmFit, testing, type="prob")
head(svmProb)

threshold = 0.2
Cred.pred = rep("Good", nrow(testing)) # All good's
Cred.pred[which(svmProb[,2] > threshold)] = "Bad" # Change the observations in the threshold as bad
head(Cred.pred)

CM = confusionMatrix(factor(Cred.pred), testing$Creditability)$table
profit.applicant <- sum(profit.unit*CM)/sum(CM)
profit.applicant
```

Only a bit better... The profit is now 0.05438672 and the accuracy is 93.52%.

The ROC

```{r}
roc.svm=roc(testing$Creditability ~ svmProb[,2])

plot(roc.knn, col="red",print.thres=TRUE) # ROC curve for knn
plot(roc.svm, add=TRUE, col='blue',print.thres=TRUE) # ROC curve for SVM
legend("bottomright", legend=c("knn", "svm"), col=c("red", "blue"), lwd=2)

roc.svm$auc

```

In out project we should do this plot with all the models, because it is a good way to discard models.

Insights? Here, even though at the beginning the blue looks better, overall we can see that the red (knn) is better. We would choose knn.

The AUC for SVM is 0.6402, while for knn is 0.7173.


## Decision trees

Using the original package: by default, rpart() function uses the Gini impurity measure to split the nodes and early stopping (pre-pruning)

```{r}
rpart.fit <- rpart(Creditability ~., method="class", data = training)
summary(rpart.fit)
```

For each node in the tree, the number of examples reaching the decision point is listed. Very arcaic output from the last century when it was developed. We are going to want to visualize the output in a more visual way, as tree branches.

Visualizing decision trees (we should use this in our project to visualize the decision tree).

```{r}
library(rpart.plot)
rpart.plot(rpart.fit, digits = 3, fallen.leaves = TRUE,
           type = 3, extra=101)
```

First question is the number of times the person has being 90 days late. The second question is the same, but with a different threshold

Each node shows:

- the predicted class
- the predicted probability of each class
- the percentage of observations in the node

Note many nodes have been removed (pruning)

To create a full tree, we can set the complexity parameter cp to 0 (split even if it does not improve the tree) and we set the minimum number of observations in a node needed to split to the smallest value of 2

Predictions

```{r}
DTPred <- predict(rpart.fit, testing, type="class")
EconomicProfit(data = data.frame(pred  = DTPred, obs = testing$Creditability))
```

A bit weak model... The profit is 0.05458627, no a very good profit1.

## C5.0

Advanced DT model, somehow it's a boosting approach 

Train and tune using Caret:

```{r}
library(c50)

grid_c50 <- expand.grid( .winnow = c(TRUE,FALSE), .trials=c(1,5,10,15), .model="tree" )

fit.c50 <- train(Creditability ~.,
                data=training,
                method="C5.0",
                metric="EconomicProfit",
                tuneGrid = grid_c50,
                trControl = ctrl)
print(fit.c50)
```

Winnowing is a feature selection step conducted before modelling and trials = number of boosting iterations. We find the most accurate one, which is the second one (0.5829473), winnow = FALSE and trials = 5. 

Predictions

```{r}
c50.pred <- predict(fit.c50, newdata=testing)
confusionMatrix(c50.pred, testing$Creditability)
EconomicProfit(data = data.frame(pred  = c50.pred, obs = testing$Creditability))              
```

A bit weak... Economic profit is 0.05798231 and accuracy is 93.52%. The most important hyperparameter here is the threshold, which we are once again going to change.

Finally, let's use probabilities: fraction of samples of the same class in a leaf

```{r}
c50.Prob = predict(fit.c50, testing, type="prob")

threshold = 0.1
Cred.pred = rep("Good", nrow(testing))
Cred.pred[which(c50.Prob[,2] > threshold)] = "Bad"
CM = confusionMatrix(factor(Cred.pred), testing$Creditability)$table
profit.applicant <- sum(profit.unit*CM)/sum(CM)
profit.applicant
```

Better! 0.07253803 economic profit and 86.56% accuracy.

The ROC

```{r}
roc.dt=roc(testing$Creditability ~ c50.Prob[,2])

plot(roc.knn, col="red",print.thres=TRUE)
plot(roc.svm, add=TRUE, col='blue',print.thres=TRUE)
plot(roc.dt, add=TRUE, col='green',print.thres=TRUE)
legend("bottomright", legend=c("knn", "svm", "dt"), col=c("red", "blue", "green"), lwd=2)

roc.dt$auc

```

We plotted the three model seen today (knn, svm, decision trees) and we can see that the best is decision trees (the threshold we should use is 0.069 and the AUC is 0.7676 = very good model) and the worst is svm (AUC = 0.6402), both can be seen very clearly.
